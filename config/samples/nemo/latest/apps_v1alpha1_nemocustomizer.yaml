apiVersion: apps.nvidia.com/v1alpha1
kind: NemoCustomizer
metadata:
  name: nemocustomizer-sample
  namespace: nemo
spec:
  wandbSecret:
    name: wandb-secret
    apiKeyKey: encryptionKey
  otel:
    enabled: true
    exporterOtlpEndpoint: http://customizer-otel-opentelemetry-collector.nemo.svc.cluster.local:4317
  databaseConfig:
    credentials:
      user: ncsuser
      secretName: customizer-pg-existing-secret
      passwordKey: password
    host: customizer-pg-postgresql.nemo.svc.cluster.local
    port: 5432
    databaseName: ncsdb
  expose:
    service:
      type: ClusterIP
      port: 8000
  image:
    repository: nvcr.io/nvidia/nemo-microservices/customizer-api
    tag: "25.04"
    pullPolicy: IfNotPresent
    pullSecrets:
      - ngc-secret
  customizerConfig: |
    namespace: nemo
    entity_store_url: http://nemoentitystore-sample.nemo.svc.cluster.local:8000
    nemo_data_store_url: http://nemodatastore-sample.nemo.svc.cluster.local:8000
    mlflow_tracking_url: http://mlflow-tracking.nemo.svc.cluster.local:80

    nemo_data_store_tools:
      image: nvcr.io/nvidia/nemo-microservices/nds-v2-huggingface-cli:25.04
      imagePullSecret: ngc-secret

    model_download_jobs:
      image: "nvcr.io/nvidia/nemo-microservices/customizer-api:25.04"
      imagePullPolicy: "IfNotPresent"
      imagePullSecrets:
        - name: ngc-secret
      ngcAPISecret: ngc-api-secret
      ngcAPISecretKey: "NGC_API_KEY"
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      # -- Time to live in seconds after the job finishes.
      ttlSecondsAfterFinished: 600
      # -- Interval in seconds to poll for model download status.
      pollIntervalSeconds: 15

    training:
      queue: "default"
      image: nvcr.io/nvidia/nemo-microservices/customizer:25.04
      imagePullSecrets:
        - name: ngc-secret
      pvc:
        size: 5Gi
        volumeAccessMode: ReadWriteOnce
      env:
        - name: LOG_LEVEL
          value: INFO
      workspace_dir: /pvc/workspace
      # volumes reference pre-existing PVCs
      volumes:
        # Model cache PVC
        - name: models
          persistentVolumeClaim:
            claimName: finetuning-ms-models-pvc
            readOnly: True
        - name: dshm
          emptyDir:
            medium: Memory
      volumeMounts:
        - name: models
          mountPath: "/mount/models"
          readOnly: True
        - name: dshm
          mountPath: "/dev/shm"

      # Network configuration for multi node training specific to CSP
      training_networking:
        - name: "NCCL_IB_SL"
          value: 0
        - name: "NCCL_IB_TC"
          value: 41
        - name: "NCCL_IB_QPS_PER_CONNECTION"
          value: 4
        - name: "UCX_TLS"
          value: TCP
        - name: "UCX_NET_DEVICES"
          value: eth0
        - name: "HCOLL_ENABLE_MCAST_ALL"
          value: 0
        - name: "NCCL_IB_GID_INDEX"
          value: 3

      tolerations: []

      container_defaults:
          imagePullPolicy: IfNotPresent

    # -- Models configuration.
    models:
      # -- Llama 3.2 3B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 3B Instruct model.
      meta/llama-3.2-3b-instruct:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI.
        model_uri: ngc://nvidia/nemo/llama-3_2-3b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama32_3b-instruct
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 1
            num_nodes: 1
            tensor_parallel_size: 1
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 3000000000
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B model.
      meta/llama-3.2-1b:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.2 1B model.
        model_uri: ngc://nvidia/nemo/llama-3_2-1b:2.0
        # -- Path where model files are stored.
        model_path: llama32_1b
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 1
            num_nodes: 1
            tensor_parallel_size: 1
          - training_type: sft
            finetuning_type: all_weights
            num_gpus: 1
            num_nodes: 1
            tensor_parallel_size: 1
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.2 1B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.2 1B Instruct model.
      meta/llama-3.2-1b-instruct:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.2 1B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_2-1b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama32_1b-instruct
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 1
            num_nodes: 1
            tensor_parallel_size: 1
          - training_type: sft
            finetuning_type: all_weights
            num_gpus: 1
            num_nodes: 1
            tensor_parallel_size: 1
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 1000000000
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3 70B Instruct model.
      meta/llama3-70b-instruct:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3-70b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3-70b-bf16
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
            num_nodes: 1
            tensor_parallel_size: 4
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 8B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 8B Instruct model.
      meta/llama-3.1-8b-instruct:
        # -- Whether to enable the model.
        enabled: true
        # -- NGC model URI for Llama 3.1 8B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_1-8b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3_1-8b-instruct_0_0_1
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 1
          - training_type: sft
            finetuning_type: all_weights
            num_gpus: 8
            num_nodes: 1
            tensor_parallel_size: 4
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 8000000000
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.1 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.1 70B Instruct model.
      meta/llama-3.1-70b-instruct:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.1 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_1-70b-instruct-nemo:2.0
        # -- Path where model files are stored.
        model_path: llama-3_1-70b-instruct_0_0_1
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
            num_nodes: 1
            tensor_parallel_size: 4
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Phi-4 model configuration.
      # @default -- This object has the following default values for the Phi-4.
      microsoft/phi-4:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Phi-4 model.
        model_uri: ngc://nvidia/nemo/phi-4:1.0
        # -- Path where model files are stored.
        model_path: phi-4
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 1
            num_nodes: 1
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 14659507200
        # -- Model precision format.
        precision: bf16
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"

      # -- Llama 3.3 70B Instruct model configuration.
      # @default -- This object has the following default values for the Llama 3.3 70B Instruct model.
      meta/llama-3.3-70b-instruct:
        # -- Whether to enable the model.
        enabled: false
        # -- NGC model URI for Llama 3.3 70B Instruct model.
        model_uri: ngc://nvidia/nemo/llama-3_3-70b-instruct:2.0
        # -- Path where model files are stored.
        model_path: llama-3_3-70b-instruct_0_0_1
        # -- Training options for different fine-tuning methods.
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
            num_nodes: 1
            tensor_parallel_size: 4
        # -- Micro batch size for training.
        micro_batch_size: 1
        # -- Maximum sequence length for input tokens.
        max_seq_length: 4096
        # -- Number of model parameters.
        num_parameters: 70000000000
        # -- Model precision format.
        precision: bf16-mixed
        # -- Template for formatting prompts.
        prompt_template: "{prompt} {completion}"
