apiVersion: apps.nvidia.com/v1alpha1
kind: NemoCustomizer
metadata:
  name: nemocustomizer-sample
  namespace: nemo
spec:
  wandbSecret:
    name: wandb-secret
    apiKeyKey: encryptionKey
  otel:
    enabled: true
    exporterOtlpEndpoint: http://customizer-otel-opentelemetry-collector.nemo.svc.cluster.local:4317
  databaseConfig:
    credentials:
      user: ncsuser
      secretName: customizer-pg-existing-secret
      passwordKey: password
    host: customizer-pg-postgresql.nemo.svc.cluster.local
    port: 5432
    databaseName: ncsdb
  expose:
    service:
      type: ClusterIP
      port: 8000
  image:
    #repository: nvcr.io/nvidia/nemo-microservices/customizer
    repository: nvcr.io/nvidian/nemo-llm/customizer
    tag: "25.04-rc10"
    pullPolicy: IfNotPresent
    pullSecrets:
      - ngc-secret
  customizerConfig: |
    namespace: nemo
    entity_store_url: http://nemoentitystore-sample.nemo.svc.cluster.local:8000
    nemo_data_store_url: http://nemodatastore-sample.nemo.svc.cluster.local:8000
    mlflow_tracking_url: http://mlflow-tracking.nemo.svc.cluster.local:80

    training:
      queue: "default"
      #image: nvcr.io/nvidia/nemo-microservices/customizer-api:25.03
      image: "nvcr.io/nvidian/nemo-llm/customizer-api:25.04-rc10"
      imagePullSecrets:
        - name: ngc-secret
      pvc:
        size: 5Gi
        volumeAccessMode: ReadWriteOnce
      env:
        - name: LOG_LEVEL
          value: INFO

      workspace_dir: /pvc/workspace
      # volumes reference pre-existing PVCs
      volumes:
        # Model cache PVC
        - name: models
          persistentVolumeClaim:
            claimName: finetuning-ms-models-pvc
            readOnly: True
        - name: dshm
          emptyDir:
            medium: Memory
      volumeMounts:
        - name: models
          mountPath: "/mount/models"
          readOnly: True
        - name: dshm
          mountPath: "/dev/shm"

      # Network configuration for multi node training specific to CSP
      training_networking:
        - name: "NCCL_IB_SL"
          value: 0
        - name: "NCCL_IB_TC"
          value: 41
        - name: "NCCL_IB_QPS_PER_CONNECTION"
          value: 4
        - name: "UCX_TLS"
          value: TCP
        - name: "UCX_NET_DEVICES"
          value: eth0
        - name: "HCOLL_ENABLE_MCAST_ALL"
          value: 0
        - name: "NCCL_IB_GID_INDEX"
          value: 3

      tolerations:
          []

      container_defaults:
          imagePullPolicy: IfNotPresent

    model_download_jobs:
      image: "nvcr.io/nvidia/nemo-microservices/customizer-api:25.03"
      imagePullPolicy: "IfNotPresent"
      imagePullSecrets:
        - name: ngc-secret
      ngcAPISecret: ngc-api-secret
      ngcAPISecretKey: "NGC_API_KEY"
      securityContext:
        fsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
      ttlSecondsAfterFinished: 600

    models:
      # NIM release 1.2 and later
      meta/llama-3.1-8b-instruct:
        enabled: false
        model_uri: ngc://nvidia/nemo/llama-3_1-8b-instruct-nemo:1.0
        model_path: llama-3_1-8b-instruct
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
          - training_type: sft
            finetuning_type: all_weights
            num_gpus: 8
            num_nodes: 1
            tensor_parallel_size: 4
        micro_batch_size: 1
        max_seq_length: 4096
        num_parameters: 8000000000
        precision: bf16
        tokenizer_path: /home/customizer/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-instruct/snapshots/c4a54320a52ed5f88b7a2f84496903ea4ff07b45
        # special_tokens:
        #   begin_of_text: <|begin_of_text|>
        #   end_of_text: <|finetune_right_pad_id|>
        #   role_start: <|start_header_id|>
        #   role_end: <|end_header_id|>
        #   label_start: ""
        #   label_end: ""
        #   turn_start: ""
        #   turn_end: <|eot_id|>

      meta/llama-3.1-70b-instruct:
        enabled: false
        model_uri: ngc://nvidia/nemo/llama-3_1-70b-instruct-nemo:1.0
        model_path: llama-3_1-70b-instruct
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
            num_nodes: 2
            tensor_parallel_size: 8
        micro_batch_size: 1
        max_seq_length: 4096
        num_parameters: 70000000000
        precision: bf16
        # special_tokens:
        #   begin_of_text: <|begin_of_text|>
        #   end_of_text: <|finetune_right_pad_id|>
        #   role_start: <|start_header_id|>
        #   role_end: <|end_header_id|>
        #   label_start: ""
        #   label_end: ""
        #   turn_start: ""
        #   turn_end: <|eot_id|>

      gpt8b-4k:
        enabled: false
        model_uri: ngc://nvidia/nemo/nemotron-3-8b-base-4k:1.0
        model_path: nemotron_3_8b_base_4k
        training_options:
          - training_type: sft
            finetuning_type: p_tuning
            num_gpus: 4
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
          - training_type: sft
            finetuning_type: all_weights
            num_gpus: 8
            tensor_parallel_size: 4
        max_seq_length: 4096
        num_parameters: 8000000000
        precision: bf16
        micro_batch_size: 1
        multinode_dataset_thresholds:
          - num_nodes: 2
            threshold: 80000

      gemma-7b:
        enabled: false
        model_uri: ngc://nvidia/nemo/gemma_7b_base:1.1
        model_path: "gemma-7b"
        training_options:
          - training_type: sft
            finetuning_type: p_tuning
            num_gpus: 2
          - training_type: sft
            finetuning_type: lora
            num_gpus: 2
        micro_batch_size: 4
        max_seq_length: 8192
        num_parameters: 7000000000
        precision: bf16

      codellama-70b:
        enabled: false
        model_path: "codellama_70b_bf16"
        training_options:
          - training_type: sft
            finetuning_type: p_tuning
            num_gpus: 4
            num_nodes: 2
            tensor_parallel_size: 8
          - training_type: sft
            finetuning_type: lora
            num_gpus: 4
            num_nodes: 2
            tensor_parallel_size: 8
        micro_batch_size: 1
        max_seq_length: 16384
        num_parameters: 70000000000
        precision: bf16

      meta/llama3-405b-instruct:
        enabled: false
        max_seq_length: 2048
        num_parameters: 405000000000
        precision: bf16
        micro_batch_size: 1
        model_path: llama-3_1-405b-instruct
        training_options:
          - training_type: sft
            finetuning_type: lora
            num_gpus: 8
            num_nodes: 3
            tensor_parallel_size: 4
        # pipeline_parallel_size: 6
        # use_sequence_parallel: false
        # special_tokens:
        #   begin_of_text: <|begin_of_text|>
        #   end_of_text: <|end_of_text|>
        #   role_start: <|start_header_id|>
        #   role_end: <|end_header_id|>
        #   label_start: ""
        #   label_end: ""
        #   turn_start: ""
        #   turn_end: <|eot_id|>

    nemo_data_store_tools:
      #image: nvcr.io/nvidia/nemo-microservices/nds-v2-huggingface-cli:25.03
      image: nvcr.io/nvidian/nemo-llm/nds-v2-huggingface-cli:25.04-rc10
      imagePullSecret: ngc-secret
