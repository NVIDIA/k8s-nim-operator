apiVersion: apps.nvidia.com/v1alpha1
kind: NIMService
metadata:
  name: meta-llama-3-2-1b-instruct
  namespace: nim-service
spec:
  annotations:
    serving.kserve.io/deploymentMode: 'RawDeployment'
  image:
    repository: nvcr.io/nim/meta/llama-3.2-1b-instruct
    tag: "1.8"
    pullPolicy: IfNotPresent
    pullSecrets:
      - ngc-secret
  authSecret: ngc-api-secret
  storage:
    nimCache: { }
    pvc:
      create: true
      size: 20Gi
      storageClass: gp3-csi
      volumeAccessMode: ReadWriteOnce
  replicas: 1
  resources:
    limits:
      nvidia.com/gpu: 1
      cpu: "12"
      memory: 32Gi
    requests:
      nvidia.com/gpu: 1
      cpu: "12"
      memory: 32Gi
  expose:
    service:
      type: ClusterIP
      port: 8000
    ingress:
      enabled: true
  tolerations:
    - effect: NoSchedule
      key: p4-gpu
      operator: Exists
  nodeSelector:
    node.kubernetes.io/instance-type: p4d.24xlarge
  livenessProbe:
    enabled: true
    probe:
      httpGet:
        path: /v1/models
        port: 8000
      initialDelaySeconds: 120
      timeoutSeconds: 300
      periodSeconds: 10
  readinessProbe:
    enabled: true
    probe:
      httpGet:
        path: /v1/models
        port: 8000
      initialDelaySeconds: 120
      timeoutSeconds: 300
      periodSeconds: 10
  startupProbe:
    enabled: true
    probe:
      httpGet:
        path: /v1/models
        port: 8000
      initialDelaySeconds: 120
      timeoutSeconds: 300
      periodSeconds: 10
  metrics:
    enabled: true
  scale:
    enabled: true
    hpa:
      minReplicas: 1
      maxReplicas: 3
      metrics:
      - type: "Resource"
        resource:
          name: "cpu"
          target:
            type: "Utilization"
            averageUtilization: 80

