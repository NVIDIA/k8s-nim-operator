---
# NIM Cache LLM-Specific NIM
apiVersion: apps.nvidia.com/v1alpha1
kind: NIMCache
metadata:
  labels:
    app.kubernetes.io/name: k8s-nim-operator
  name: meta-llama-3-2-1b-instruct
  namespace: nim-service
spec:
  source:
    ngc:
      modelPuller: nvcr.io/nim/meta/llama-3.2-1b-instruct:1.8
      pullSecret: ngc-secret
      authSecret: ngc-api-secret
      model:
        engine: "tensorrt"
        tensorParallelism: "1"
  storage:
    pvc:
      create: true
      size: "50Gi"
      volumeAccessMode: ReadWriteOnce

---
# NIM Service LLM-Specific NIM
apiVersion: apps.nvidia.com/v1alpha1
kind: NIMService
metadata:
  name: meta-llama-3-2-1b-instruct
  namespace: nim-service
spec:
  inferencePlatform: kserve
  annotations:
    # Knative concurrency-based autoscaling (default).
    autoscaling.knative.dev/class: kpa.autoscaling.knative.dev
    autoscaling.knative.dev/metric: concurrency
    # Target 10 requests in-flight per pod.
    autoscaling.knative.dev/target: "10"
    # Disable scale to zero with a min scale of 1.
    autoscaling.knative.dev/min-scale: "1"
    # Limit scaling to 100 pods.
    autoscaling.knative.dev/max-scale: "10"
  image:
    repository: nvcr.io/nim/meta/llama-3.2-1b-instruct
    tag: "1.8"
    pullPolicy: IfNotPresent
    pullSecrets:
      - ngc-secret
  authSecret: ngc-api-secret
  storage:
    nimCache:
      name: meta-llama-3-2-1b-instruct
      profile: ''
  resources:
    limits:
      nvidia.com/gpu: 1
      cpu: "12"
      memory: 32Gi
    requests:
      nvidia.com/gpu: 1
      cpu: "4"
      memory: 6Gi
  replicas: 1
  expose:
    service:
      type: ClusterIP
      port: 8000
